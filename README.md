# ğŸ§  Multi-Threaded AI Chatbot Server

![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)
![FAISS](https://img.shields.io/badge/FAISS-Enabled-orange.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Active-success.svg)

A **multi-threaded conversational AI system** that allows multiple clients to chat concurrently with an intelligent server.  
Each client interacts in real time with a **Contextual Knowledge Base (FAISS-powered retriever)** that falls back to a **Large Language Model (LLM)** when no good match is found.

---

## âš™ï¸ Features

- Multi-threaded socket server â€” handles multiple clients concurrently  
- Real-time message exchange over TCP  
- Semantic QA retrieval with **FAISS + SentenceTransformer**  
- **LLM fallback** via OpenRouter (Mistral 7B, Gemini, etc.)  
- Persistent embeddings for fast startup  
- Modular, extensible architecture

---

## ğŸ§© System Overview
### Architecture
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚              Server (AI Brain)           â”‚
      â”‚------------------------------------------â”‚
      â”‚ ContextualKnowledgeBase (FAISS + LLM)    â”‚
      â”‚ Thread 1 â”€ handle_client(conn_1)         â”‚
      â”‚ Thread 2 â”€ handle_client(conn_2)         â”‚
      â”‚ Thread n â”€ handle_client(conn_n)         â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†‘                    â†‘
               â”‚                    â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     Client #1        â”‚  â”‚   Client #2   â”‚
    â”‚ (Socket + Input Loop)â”‚  â”‚ (Socket Loop) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

---

## ğŸ§± Core Components

### `multi_threaded_server.py`
- Initializes TCP socket (`127.0.0.1:5000`)
- Loads **ContextualKnowledgeBase** (FAISS index)
- Spawns new thread for every incoming client  
- Handles message receive, query, and send cycle  
- Each client is isolated and concurrent

### `client.py`
- Connects to server and starts a user input loop  
- Sends text queries and receives responses  
- Supports `"exit"` command to disconnect gracefully

### `contextual_brain.py`
- Loads the WikiQA dataset (`wikiqa_final.csv`)  
- Uses **SentenceTransformer ("multi-qa-MiniLM-L6-cos-v1")** to embed questions  
- Builds or loads a **FAISS inner product index** for fast similarity search  
- Returns best answers above threshold; otherwise calls **LLM fallback**

### `llm_fallback.py`
- Calls OpenRouter API for Mistral-7B-Instruct responses  
- If no API key or network, gracefully degrades to local text stub  

### `test_bot.py`
- CLI test harness for querying the ContextualKnowledgeBase directly  
- Useful for validating retrieval + LLM logic before server integration  

---

## ğŸ§  Query Flow

1. Client sends message â†’ Server receives via socket  
2. Server passes message to `ContextualKnowledgeBase.query()`  
3. System embeds the query and retrieves top matches from FAISS  
4. If similarity score â‰¥ 0.6 â†’ returns best answers  
5. Else â†’ calls `get_llm_response()` (LLM fallback)  
6. Server sends response back to client  

---

## ğŸ§¾ Example Interaction

**Client**

    Connected to server at 127.0.0.1:5000
    You: What is machine learning?
    Bot: (Top score: 0.845)

    (0.845) Machine learning is a subset of AI where systems learn patterns from data.
    You: Who founded OpenAI?
    Bot: OpenAI was founded by Elon Musk, Sam Altman, and others in 2015.
    You: exit
  
**Server Console**

    ğŸš€ Server running on 127.0.0.1:5000
    ğŸ“š Loading dataset: data/wikiqa_final.csv
    ğŸ“¦ Loading existing FAISS index...
    âœ… ContextualKnowledgeBase ready with 1040 entries.
    ğŸ§© Connected with ('127.0.0.1', 53421)
    [('127.0.0.1', 53421)] User: What is machine learning?
    ğŸ‘¥ Active connections: 1
    ğŸ”Œ Disconnected ('127.0.0.1', 53421)

---

## ğŸ“¦ Repository Structure

    CHATBOT
    â”‚   faiss_test.py
    â”‚   getwikiqa.py
    â”‚   main.py
    â”‚   README.md
    â”‚   requirements.txt
    â”‚   __init__.py
    â”‚
    â”œâ”€â”€â”€data
    â”‚   â”‚   faq_data.csv
    â”‚   â”‚
    â”‚   â””â”€â”€â”€wikiqa
    â”‚       â”‚   wikiqa.csv
    â”‚       â”‚   wikiqa_clean.csv
    â”‚       â”‚   wikiqa_final.csv
    â”‚       â”‚
    â”‚       â”œâ”€â”€â”€processed
    â”‚       â”‚       prepare_wikiqa.py
    â”‚       â”‚
    â”‚       â””â”€â”€â”€raw
    â”œâ”€â”€â”€embeddings
    â”‚       faiss_index.bin
    â”‚       questions.npy
    â”‚       wikiqa_faiss.index
    â”‚
    â”œâ”€â”€â”€src
    â”‚   â”‚   bot_brain.py
    â”‚   â”‚   contextual_brain.py
    â”‚   â”‚   embedder.py
    â”‚   â”‚   llm_fallback.py
    â”‚   â”‚   multi_threaded_server.py
    â”‚   â”‚   retriever.py
    â”‚   â”‚   __init__.py
    â”‚   â”‚
    â”‚   â””â”€â”€â”€__pycache__
    â”‚           bot_brain.cpython-313.pyc
    â”‚           contextual_brain.cpython-313.pyc
    â”‚           llm_fallback.cpython-313.pyc
    â”‚           multi_threaded_server.cpython-313.pyc
    â”‚           __init__.cpython-313.pyc
    â”‚
    â”œâ”€â”€â”€tests
    â”‚   â”‚   client.py
    â”‚   â”‚   test_bot.py
    â”‚   â”‚   test_faiss.py
    â”‚   â”‚   __init__.py
    â”‚   â”‚
    â”‚   â””â”€â”€â”€__pycache__
    â”‚           test_bot.cpython-313.pyc
    â”‚           __init__.cpython-313.pyc
    â”‚
    â””â”€â”€â”€__pycache__
            __init__.cpython-313.pyc
  
---

## ğŸ§  Setup & Usage

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/<your-username>/multi-threaded-chatbot.git
cd multi-threaded-chatbot
pip install sentence-transformers faiss-cpu pandas numpy requests scikit-learn
python multi_threaded_server.py
python client.py
```
You can start multiple clients simultaneously â€” each runs in its own thread on the server.

---
ğŸ” Environment Variable (Optional)

To use live LLM fallback, set your OpenRouter API key:
```bash
setx OPENROUTER_API_KEY "sk-or-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
```
---

ğŸ§® Future Enhancements

| Area                     | Planned Upgrade                                    |
| ------------------------ | -------------------------------------------------- |
| **Async Architecture**   | Replace threads with `asyncio` for scalable I/O    |
| **User Authentication**  | Add unique user IDs and session history            |
| **Logging & Analytics**  | Store conversation logs for NLP analytics          |
| **WebSocket Layer**      | Build browser-accessible version with `websockets` |
| **GUI Client**           | Add Tkinter or React-based chat interface          |
| **Fine-tuned QA Models** | Integrate custom domain QA models                  |

---

ğŸ‘¤ Author

    Developer: Paramhans Mishra

    Email: param110045@gmail.com

    GitHub Portfolio: github.com/paramhansmishra

Explore my GitHub for in-depth AI, ML, and system design projects â€” including CNN-based ingredient analysis, face recognition, and retrieval-augmented bots.
---
